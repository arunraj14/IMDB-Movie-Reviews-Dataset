{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "In this kernel, I work on the IMDB movie review dataset and to predict sentiments from the movie review. I will visualize the dataset using seaborn and will perform text preprocessing. It is an imbalanced dataset, so i will try to solve that problem as well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries to read & plot data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# sklearn libraries for prediction and evaluation\n",
    "from sklearn import model_selection, preprocessing, metrics, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# imblearn for balancing dataset\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline,make_pipeline\n",
    "\n",
    "# nltk for text processing\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Remove Special Charactors\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set: (27502, 2) 27502\n"
     ]
    }
   ],
   "source": [
    "# Read dataset from local machine\n",
    "df = pd.read_csv(r'C:\\Users\\kvbt565\\Desktop\\IMDB Usecase\\IMDB Dataset\\IMDB Dataset.csv')\n",
    "print(\"Data Set:\"% df.columns, df.shape, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>27502</td>\n",
       "      <td>27502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>27358</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               27502     27502\n",
       "unique                                              27358         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  90.90247981964949 %\n",
      "Negative:  9.09752018035052 %\n"
     ]
    }
   ],
   "source": [
    "# Percentage of Positive/Negative\n",
    "print(\"Positive: \", df.sentiment.value_counts()[0]/len(df)*100,\"%\")\n",
    "print(\"Negative: \", df.sentiment.value_counts()[1]/len(df)*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we have an imbalanced dataset and need to consider balancing dataset before prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "colors = sns.color_palette('deep')\n",
    "\n",
    "plt.figure(figsize=(8,4), tight_layout=True)\n",
    "plt.bar(x=['Positive', 'Negative'],\n",
    "        height=df['sentiment'].value_counts(),\n",
    "        color=colors[:2])\n",
    "plt.title('Sentiment')\n",
    "plt.savefig('sentiment.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_review = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer to update the root word\n",
    "porter=PorterStemmer()\n",
    "\n",
    "# Stopwords except 'from'\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.remove('from')\n",
    "#all_stopwords.extend(['the' , 'to' , 'it' , 'i' , 't'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change text to lowercase\n",
    "movie_review['review'] = movie_review['review'].str.lower()\n",
    "\n",
    "# Removing xml related characters\n",
    "movie_review['review'] = movie_review['review'].apply(lambda x: BeautifulSoup(x, 'lxml').get_text())\n",
    "\n",
    "# Removing Non-Digit and Non-letter parts\n",
    "movie_review['review'] = movie_review['review'].replace(to_replace=r\"[^A-Za-z0-9]+\", value=r\" \", regex=True)\n",
    "#movie_review['review'] = movie_review['review'].apply(lambda x: x.translate(str.maketrans('','',string.punctuation)))\n",
    "\n",
    "# Removing standalone numbers\n",
    "movie_review['review'] = movie_review['review'].apply(lambda x: ' '.join([word for word in x.split() if not word.isdigit()]))\n",
    "\n",
    "# Removing stopwords \n",
    "movie_review['review'] = movie_review['review'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (all_stopwords)]))\n",
    "\n",
    "# Updating the text to its root word\n",
    "movie_review['review'] = movie_review['review'].apply(lambda x :' '.join([porter.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the Dataset into feature & target dataframes\n",
    "X = movie_review['review']\n",
    "Y = movie_review['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Train Test Data\n",
    "\n",
    "Splitting 80% of data as trian data & 20% of data as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=0.20, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of positive class in trainig data set : 19978\n",
      "Count of negative class in trainig data set : 2023\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of positive class in trainig data set :\" ,(sum(y_train==1)))\n",
    "print(\"Count of negative class in trainig data set :\",format(sum(y_train==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of positive class in testing data set : 5022\n",
      "Count of negative class in testing data set : 479\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of positive class in testing data set :\" ,(sum(y_test==1)))\n",
    "print(\"Count of negative class in testing data set :\",format(sum(y_test==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great concert featur best song band year career lee lifeson peart anim fun stage deliv great show said think audio record botch mix consol mike process fault boy howev even mention show setup last minut band crew obvious best put show public consumpt neil snare sound distant geddi bass good harmon content bottom end held concert wait better occas buy dvd hardcor rushian qualiti lack audio depart\n"
     ]
    }
   ],
   "source": [
    "print(x_test[18887])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(y_train[:] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(r'test_imdb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>18887</td>\n",
       "      <td>great concert featur best song band year caree...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>15369</td>\n",
       "      <td>scale suicid sweetheart got from from everyon ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13761</td>\n",
       "      <td>six month old babi home time time fight sleep ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12189</td>\n",
       "      <td>came sneak preview film laugh everi sec end fu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5522</td>\n",
       "      <td>whilst love haunt hous movi amityvil poltergei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5496</td>\n",
       "      <td>17214</td>\n",
       "      <td>noth like moonstruck from new york italian fam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5497</td>\n",
       "      <td>11408</td>\n",
       "      <td>great film mccartney beatl fan splendid time g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5498</td>\n",
       "      <td>2993</td>\n",
       "      <td>must admit one favorit horror film time uniqu ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5499</td>\n",
       "      <td>13631</td>\n",
       "      <td>clich avoid one film main achiev hear vagu out...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>17000</td>\n",
       "      <td>chucki back time time help jennif tilli littl ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5501 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                             review  sentiment\n",
       "0     18887  great concert featur best song band year caree...          1\n",
       "1     15369  scale suicid sweetheart got from from everyon ...          1\n",
       "2     13761  six month old babi home time time fight sleep ...          1\n",
       "3     12189  came sneak preview film laugh everi sec end fu...          1\n",
       "4      5522  whilst love haunt hous movi amityvil poltergei...          1\n",
       "...     ...                                                ...        ...\n",
       "5496  17214  noth like moonstruck from new york italian fam...          1\n",
       "5497  11408  great film mccartney beatl fan splendid time g...          1\n",
       "5498   2993  must admit one favorit horror film time uniqu ...          1\n",
       "5499  13631  clich avoid one film main achiev hear vagu out...          1\n",
       "5500  17000  chucki back time time help jennif tilli littl ...          1\n",
       "\n",
       "[5501 rows x 3 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.DataFrame(x_test)\n",
    "test.reset_index(inplace=True)\n",
    "test['sentiment'] = y_test.tolist()\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vectorization\n",
    "\n",
    "We need to convert the text into numerical values before passing it to the Model for training & prediction. Basic algorithms like one-hot encoder, Count Vectorizer can be used but i am using Tf-Idf vectorizer since it performs much better and provides a logical vectorization.\n",
    "\n",
    "We can also use other embedding techinques like Word2vec, Glove Or BERT which will give us the context based importance of the words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Tf-idf Vectorization\n",
    "\n",
    "Term Frequency             --> tf(t,d) = count of t in d / number of words in d <br>\n",
    "Document Frequency         --> df(t)   = occurrence of t in documents  <br>\n",
    "Inverse Document Frequency --> idf(t) = log(N/(df + 1)) <br>\n",
    "TF - IDF                   --> tf-idf(t, d) = tf(t, d) * log(N/(df + 1)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
    "xtrain_tfidf =  tfidf_vect.fit_transform(x_train)\n",
    "xtest_tfidf =  tfidf_vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (22001, 48202)\n",
      "Tfidf_test: (5501, 48202)\n"
     ]
    }
   ],
   "source": [
    "print('Tfidf_train:',xtrain_tfidf.shape)\n",
    "print('Tfidf_test:',xtest_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Balancing dataset & Model Selection\n",
    "\n",
    "Here I am trying to explore the right algorithm for balancing the dataset and also the right model for prediction. Also, i used F1 score to evaluate the performance for each model as it gives better results for imbalanced dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the F1 Score\n",
    "def train_model(classifier, vector_train, tgt, vector_test):    \n",
    "    classifier.fit(vector_train, tgt)    \n",
    "    predictions = classifier.predict(vector_test)\n",
    "    return metrics.f1_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Baseline, WordLevel TFIDF:  0.975349632759835\n",
      "SVM Baseline, WordLevel TFIDF:  0.9765492465821775\n",
      "Decision Tree Baseline, WordLevel TFIDF:  0.9493607168025773\n"
     ]
    }
   ],
   "source": [
    "# Random Over Sampling\n",
    "ros = RandomOverSampler(random_state=777)\n",
    "ros_xtrain_tfidf, ros_train_y = ros.fit_resample(xtrain_tfidf, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "Acc_Ros = train_model(LogisticRegression(random_state=0,multi_class='multinomial'),ros_xtrain_tfidf, ros_train_y, xtest_tfidf)\n",
    "print (\"LR Baseline, WordLevel TFIDF: \", Acc_Ros)\n",
    "\n",
    "# Support Vector classification\n",
    "Acc_Ros = train_model(svm.LinearSVC(), ros_xtrain_tfidf, ros_train_y, xtest_tfidf)\n",
    "print (\"SVM Baseline, WordLevel TFIDF: \", Acc_Ros)\n",
    "\n",
    "# Decision Tree classifier\n",
    "Acc_Ros = train_model(DecisionTreeClassifier(), ros_xtrain_tfidf, ros_train_y, xtest_tfidf)\n",
    "print (\"Decision Tree Baseline, WordLevel TFIDF: \", Acc_Ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR ORIGINAL, WordLevel TFIDF:  0.9596136060014387\n",
      "SVM ROS, WordLevel TFIDF:  0.9566831683168316\n",
      "Decision Tree Baseline, WordLevel TFIDF:  0.8739403280854343\n"
     ]
    }
   ],
   "source": [
    "# Random Under Sampling\n",
    "rus = RandomUnderSampler(random_state=777)\n",
    "rus_xtrain_tfidf, rus_train_y = rus.fit_resample(xtrain_tfidf, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "Acc_Rus = train_model(LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),rus_xtrain_tfidf, rus_train_y, xtest_tfidf)\n",
    "print(\"LR ORIGINAL, WordLevel TFIDF: \", Acc_Rus)\n",
    "     \n",
    "# Support Vector classification\n",
    "Acc_Rus = train_model(svm.LinearSVC(),rus_xtrain_tfidf, rus_train_y, xtest_tfidf)\n",
    "print (\"SVM ROS, WordLevel TFIDF: \", Acc_Rus)\n",
    "\n",
    "# Decision Tree classifier\n",
    "Acc_Rus = train_model(DecisionTreeClassifier(), rus_xtrain_tfidf, rus_train_y, xtest_tfidf)\n",
    "print (\"Decision Tree Baseline, WordLevel TFIDF: \", Acc_Rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR ORIGINAL, WordLevel TFIDF:  0.9812524930195452\n",
      "SVM ROS, WordLevel TFIDF:  0.9797216699801192\n",
      "Decision Tree Baseline, WordLevel TFIDF:  0.9338257925515543\n"
     ]
    }
   ],
   "source": [
    "# SMOTE(Synthetic Minority Oversampling Technique)\n",
    "smote = SMOTE(sampling_strategy = 1 ,k_neighbors = 3, random_state=1)   \n",
    "smot_train_tfidf, smot_train_y = smote.fit_resample(xtrain_tfidf, y_train)\n",
    "\n",
    "# Logistic Regression\n",
    "Smot_Rus = train_model(LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),smot_train_tfidf, smot_train_y, xtest_tfidf)\n",
    "print(\"LR ORIGINAL, WordLevel TFIDF: \", Smot_Rus)\n",
    "     \n",
    "# Support Vector classification    \n",
    "Smot_Rus = train_model(svm.LinearSVC(),smot_train_tfidf, smot_train_y, xtest_tfidf)\n",
    "print (\"SVM ROS, WordLevel TFIDF: \", Smot_Rus)\n",
    "\n",
    "Smot_Rus = train_model(DecisionTreeClassifier(), smot_train_tfidf, smot_train_y, xtest_tfidf)\n",
    "print (\"Decision Tree Baseline, WordLevel TFIDF: \", Smot_Rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results sampling with SMOTE and prediction with Logistic Regression gives a better result. Hence, confirming the Accuracy as well for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the Accuracy Score\n",
    "def train_model(classifier, vector_train, tgt, vector_test):    \n",
    "    classifier.fit(vector_train, tgt)    \n",
    "    predictions = classifier.predict(vector_test)\n",
    "    return metrics.accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR ORIGINAL, WordLevel TFIDF:  0.9658243955644429\n",
      "SVM ROS, WordLevel TFIDF:  0.9629158334848209\n",
      "Decision Tree Baseline, WordLevel TFIDF:  0.8807489547355026\n"
     ]
    }
   ],
   "source": [
    "# SMOTE(Synthetic Minority Oversampling Technique) -- Accuraccy\n",
    "smote = SMOTE(sampling_strategy = 1 ,k_neighbors = 3, random_state=1)   \n",
    "smot_train_tfidf, smot_train_y = smote.fit_resample(xtrain_tfidf, y_train.ravel())\n",
    "\n",
    "# Logistic Regression\n",
    "Smot_Rus = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),smot_train_tfidf, smot_train_y, xtest_tfidf)\n",
    "print(\"LR ORIGINAL, WordLevel TFIDF: \", Smot_Rus)\n",
    "     \n",
    "# Support Vector classification    \n",
    "Smot_Rus = train_model(svm.LinearSVC(),smot_train_tfidf, smot_train_y, xtest_tfidf)\n",
    "print (\"SVM ROS, WordLevel TFIDF: \", Smot_Rus)\n",
    "\n",
    "# Decision Tree classifier\n",
    "Smot_Rus = train_model(DecisionTreeClassifier(), smot_train_tfidf, smot_train_y, xtest_tfidf)\n",
    "print (\"Decision Tree Baseline, WordLevel TFIDF: \", Smot_Rus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that Logistic Regression has an edge over SVM with SMOTE sampling. Now, its time to tune the Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(max_iter=1000),\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10],\n",
       "                         'multi_class': ['multinomial'], 'random_state': [0],\n",
       "                         'solver': ['lbfgs']})"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the parameters\n",
    "params = {'solver':['lbfgs'],'C': [0.001,0.01,0.1,1,10],'random_state':[0],'multi_class':['multinomial']}\n",
    "\n",
    "# Run Grid Search against different input Parameters\n",
    "svc = LogisticRegression(max_iter=1000)\n",
    "svc_grid = model_selection.GridSearchCV(svc,params, cv = 5)\n",
    "svc_grid.fit(smot_train_tfidf, smot_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'multi_class': 'multinomial', 'random_state': 0, 'solver': 'lbfgs'}\n",
      "LogisticRegression(C=10, max_iter=1000, multi_class='multinomial',\n",
      "                   random_state=0)\n"
     ]
    }
   ],
   "source": [
    "# Grid Search gives the best parameters for the model \n",
    "print(svc_grid.best_params_)\n",
    "print(svc_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Model & Pickling\n",
    "\n",
    "Logistic Regression gives us better results, so trying to create a pipeline with imblearn and store the model as a pipeline to be predicted seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identified algorithms with their parameters\n",
    "tfidf = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=100000)\n",
    "smote = SMOTE(sampling_strategy = 1 ,k_neighbors = 3, random_state=1)   \n",
    "lr = LogisticRegression(random_state=0,C=10,max_iter=1000, solver='lbfgs',multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an imblearn pipeline\n",
    "pipe = make_pipeline(tfidf, smote, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(max_features=100000, token_pattern='\\\\w{1,}')),\n",
       "                ('smote',\n",
       "                 SMOTE(k_neighbors=3, random_state=1, sampling_strategy=1)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=10, max_iter=1000,\n",
       "                                    multi_class='multinomial',\n",
       "                                    random_state=0))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the data against the pipeline\n",
    "model = pipe.fit(x_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pickle file\n",
    "pickle.dump(model, open('classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Evaluating the pipeline with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the pipeline for test features\n",
    "lr_predict=model.predict(x_test)\n",
    "print(lr_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy Score : 0.9592801308852936\n",
      "LR Precision Score : 0.9794164668265388\n",
      "LR Recall Score : 0.9759060135404222\n",
      "LR F1 Score : 0.9776580889686813\n"
     ]
    }
   ],
   "source": [
    "# Metrics from the pipeline\n",
    "lf_accuracy=accuracy_score(y_test,lr_predict)\n",
    "print(\"LR Accuracy Score :\",lf_accuracy)\n",
    "\n",
    "lf_precision=precision_score(y_test,lr_predict)\n",
    "print(\"LR Precision Score :\",lf_precision)\n",
    "\n",
    "lf_recall=recall_score(y_test,lr_predict)\n",
    "print(\"LR Recall Score :\",lf_recall)\n",
    "\n",
    "lf_f1=f1_score(y_test,lr_predict)\n",
    "print(\"LR F1 Score :\",lf_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alabaster==0.7.12\n",
      "anaconda-client==1.7.2\n",
      "anaconda-navigator==1.9.7\n",
      "anaconda-project==0.8.3\n",
      "asn1crypto==1.0.1\n",
      "astroid==2.3.1\n",
      "astropy==3.2.1\n",
      "atomicwrites==1.3.0\n",
      "attrs==19.2.0\n",
      "Babel==2.7.0\n",
      "backcall==0.1.0\n",
      "backports.functools-lru-cache==1.5\n",
      "backports.os==0.1.1\n",
      "backports.shutil-get-terminal-size==1.0.0\n",
      "backports.tempfile==1.0\n",
      "backports.weakref==1.0.post1\n",
      "beautifulsoup4==4.8.0\n",
      "bitarray==1.0.1\n",
      "bkcharts==0.2\n",
      "bleach==3.1.0\n",
      "bokeh==1.3.4\n",
      "boto==2.49.0\n",
      "Bottleneck==1.2.1\n",
      "certifi==2019.9.11\n",
      "cffi==1.12.3\n",
      "chardet==3.0.4\n",
      "Click==7.0\n",
      "cloudpickle==1.2.2\n",
      "clyent==1.2.2\n",
      "colorama==0.4.1\n",
      "comtypes==1.1.7\n",
      "conda==4.7.12\n",
      "conda-build==3.18.9\n",
      "conda-package-handling==1.6.0\n",
      "conda-verify==3.4.2\n",
      "contextlib2==0.6.0\n",
      "cryptography==2.7\n",
      "cycler==0.10.0\n",
      "Cython==0.29.13\n",
      "cytoolz==0.10.0\n",
      "dask==2.5.2\n",
      "decorator==4.4.0\n",
      "defusedxml==0.6.0\n",
      "distributed==2.5.2\n",
      "docopt==0.6.2\n",
      "docutils==0.15.2\n",
      "entrypoints==0.3\n",
      "et-xmlfile==1.0.1\n",
      "fastcache==1.1.0\n",
      "filelock==3.0.12\n",
      "Flask==1.1.1\n",
      "fsspec==0.5.2\n",
      "future==0.17.1\n",
      "gevent==1.4.0\n",
      "glob2==0.7\n",
      "greenlet==0.4.15\n",
      "h5py==2.9.0\n",
      "HeapDict==1.0.1\n",
      "html5lib==1.0.1\n",
      "idna==2.8\n",
      "imageio==2.6.0\n",
      "imagesize==1.1.0\n",
      "imbalanced-learn==0.9.0\n",
      "imblearn==0.0\n",
      "importlib-metadata==0.23\n",
      "ipykernel==5.1.2\n",
      "ipython==7.8.0\n",
      "ipython-genutils==0.2.0\n",
      "ipywidgets==7.5.1\n",
      "isort==4.3.21\n",
      "itsdangerous==1.1.0\n",
      "jdcal==1.4.1\n",
      "jedi==0.15.1\n",
      "Jinja2==2.10.3\n",
      "joblib==0.13.2\n",
      "json5==0.8.5\n",
      "jsonschema==3.0.2\n",
      "jupyter==1.0.0\n",
      "jupyter-client==5.3.3\n",
      "jupyter-console==6.0.0\n",
      "jupyter-core==4.5.0\n",
      "jupyterlab==1.1.4\n",
      "jupyterlab-server==1.0.6\n",
      "keyring==18.0.0\n",
      "kiwisolver==1.1.0\n",
      "lazy-object-proxy==1.4.2\n",
      "libarchive-c==2.8\n",
      "llvmlite==0.29.0\n",
      "locket==0.2.0\n",
      "lxml==4.4.1\n",
      "MarkupSafe==1.1.1\n",
      "matplotlib==3.1.1\n",
      "mccabe==0.6.1\n",
      "menuinst==1.4.16\n",
      "mistune==0.8.4\n",
      "mkl-fft==1.0.14\n",
      "mkl-random==1.1.0\n",
      "mkl-service==2.3.0\n",
      "mock==3.0.5\n",
      "more-itertools==7.2.0\n",
      "mpmath==1.1.0\n",
      "msgpack==0.6.1\n",
      "multipledispatch==0.6.0\n",
      "navigator-updater==0.2.1\n",
      "nbconvert==5.6.0\n",
      "nbformat==4.4.0\n",
      "networkx==2.3\n",
      "nltk==3.4.5\n",
      "nose==1.3.7\n",
      "notebook==6.0.1\n",
      "numba==0.45.1\n",
      "numexpr==2.7.0\n",
      "numpy==1.16.5\n",
      "numpydoc==0.9.1\n",
      "olefile==0.46\n",
      "openpyxl==3.0.0\n",
      "packaging==19.2\n",
      "pandas==0.25.1\n",
      "pandocfilters==1.4.2\n",
      "parso==0.5.1\n",
      "partd==1.0.0\n",
      "path.py==12.0.1\n",
      "pathlib2==2.3.5\n",
      "patsy==0.5.1\n",
      "pep8==1.7.1\n",
      "pickleshare==0.7.5\n",
      "Pillow==6.2.0\n",
      "pipreqs==0.4.11\n",
      "pipreqsnb==0.2.4\n",
      "pkginfo==1.5.0.1\n",
      "pluggy==0.13.0\n",
      "ply==3.11\n",
      "prometheus-client==0.7.1\n",
      "prompt-toolkit==2.0.10\n",
      "psutil==5.6.3\n",
      "py==1.8.0\n",
      "pycodestyle==2.5.0\n",
      "pycosat==0.6.3\n",
      "pycparser==2.19\n",
      "pycrypto==2.6.1\n",
      "pycurl==7.43.0.3\n",
      "pyflakes==2.1.1\n",
      "Pygments==2.4.2\n",
      "pylint==2.4.2\n",
      "pymongo==3.11.4\n",
      "pyodbc==4.0.27\n",
      "pyOpenSSL==19.0.0\n",
      "pyparsing==2.4.2\n",
      "pyreadline==2.1\n",
      "pyrsistent==0.15.4\n",
      "PySocks==1.7.1\n",
      "pytest==5.2.1\n",
      "pytest-arraydiff==0.3\n",
      "pytest-astropy==0.5.0\n",
      "pytest-doctestplus==0.4.0\n",
      "pytest-openfiles==0.4.0\n",
      "pytest-remotedata==0.3.2\n",
      "python-dateutil==2.8.0\n",
      "pytz==2019.3\n",
      "PyWavelets==1.0.3\n",
      "pywin32==223\n",
      "pywinpty==0.5.5\n",
      "PyYAML==5.1.2\n",
      "pyzmq==18.1.0\n",
      "QtAwesome==0.6.0\n",
      "qtconsole==4.5.5\n",
      "QtPy==1.9.0\n",
      "requests==2.22.0\n",
      "rope==0.14.0\n",
      "ruamel-yaml==0.15.46\n",
      "scikit-image==0.15.0\n",
      "scikit-learn==1.0.2\n",
      "scipy==1.3.1\n",
      "seaborn==0.9.0\n",
      "Send2Trash==1.5.0\n",
      "simplegeneric==0.8.1\n",
      "singledispatch==3.4.0.3\n",
      "six==1.12.0\n",
      "snowballstemmer==2.0.0\n",
      "sortedcollections==1.1.2\n",
      "sortedcontainers==2.1.0\n",
      "soupsieve==1.9.3\n",
      "Sphinx==2.2.0\n",
      "sphinxcontrib-applehelp==1.0.1\n",
      "sphinxcontrib-devhelp==1.0.1\n",
      "sphinxcontrib-htmlhelp==1.0.2\n",
      "sphinxcontrib-jsmath==1.0.1\n",
      "sphinxcontrib-qthelp==1.0.2\n",
      "sphinxcontrib-serializinghtml==1.1.3\n",
      "sphinxcontrib-websupport==1.1.2\n",
      "spyder==3.3.6\n",
      "spyder-kernels==0.5.2\n",
      "SQLAlchemy==1.3.9\n",
      "statsmodels==0.10.1\n",
      "sympy==1.4\n",
      "tables==3.5.2\n",
      "tblib==1.4.0\n",
      "terminado==0.8.2\n",
      "testpath==0.4.2\n",
      "threadpoolctl==3.1.0\n",
      "toolz==0.10.0\n",
      "tornado==6.0.3\n",
      "tqdm==4.36.1\n",
      "traitlets==4.3.3\n",
      "unicodecsv==0.14.1\n",
      "urllib3==1.24.2\n",
      "wcwidth==0.1.7\n",
      "webencodings==0.5.1\n",
      "Werkzeug==0.16.0\n",
      "widgetsnbextension==3.5.1\n",
      "win-inet-pton==1.1.0\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "wrapt==1.11.2\n",
      "xgboost==1.5.2\n",
      "xlrd==1.2.0\n",
      "XlsxWriter==1.2.1\n",
      "xlwings==0.15.10\n",
      "xlwt==1.3.0\n",
      "yarg==0.1.9\n",
      "zict==1.0.0\n",
      "zipp==0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-43ddcc249387>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pandas' is not defined"
     ]
    }
   ],
   "source": [
    "print(pandas.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
